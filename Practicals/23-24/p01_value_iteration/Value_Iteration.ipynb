{"cells":[{"cell_type":"markdown","id":"73118f47","metadata":{"id":"73118f47"},"source":["# Value Iteration"]},{"cell_type":"markdown","id":"53ec0869","metadata":{"id":"53ec0869"},"source":["An example over 1000 iterations<br><br>\n","![](imgs/value_iteration_1.png)\n","![](imgs/value_iteration_2.png)\n","![](imgs/value_iteration_3.png)\n","![](imgs/value_iteration_4.png)\n","![](imgs/value_iteration_5.png)\n","![](imgs/value_iteration_6.png)"]},{"cell_type":"markdown","id":"81409513","metadata":{},"source":["## Install Gymnasium"]},{"cell_type":"code","execution_count":2,"id":"SUkadH__i-OK","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10079,"status":"ok","timestamp":1694424116046,"user":{"displayName":"Andrea Fanti","userId":"12083130687901228611"},"user_tz":-120},"id":"SUkadH__i-OK","outputId":"67664a67-6594-40d5-e998-ffd0ae1df00f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: gymnasium in c:\\users\\gianm\\anaconda3\\lib\\site-packages (0.29.1)\n","Requirement already satisfied: numpy>=1.21.0 in c:\\users\\gianm\\anaconda3\\lib\\site-packages (from gymnasium) (1.24.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\gianm\\anaconda3\\lib\\site-packages (from gymnasium) (2.0.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\gianm\\anaconda3\\lib\\site-packages (from gymnasium) (4.8.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\gianm\\anaconda3\\lib\\site-packages (from gymnasium) (0.0.4)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\gianm\\anaconda3\\lib\\site-packages (from gymnasium) (6.0.1)\n","Requirement already satisfied: zipp>=0.5 in c:\\users\\gianm\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.8.0)\n"]}],"source":["!pip install gymnasium"]},{"cell_type":"markdown","id":"abca06a2","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":3,"id":"ade4eb08","metadata":{},"outputs":[],"source":["import os\n","import random\n","import time\n","\n","import gymnasium as gym\n","from gymnasium import spaces\n","import numpy as np"]},{"cell_type":"markdown","id":"0009627b","metadata":{},"source":["## Initialize 2D environment"]},{"cell_type":"code","execution_count":4,"id":"2523810d","metadata":{"executionInfo":{"elapsed":241,"status":"ok","timestamp":1694424306244,"user":{"displayName":"Andrea Fanti","userId":"12083130687901228611"},"user_tz":-120},"id":"2523810d"},"outputs":[],"source":["# Custom 2D GridWorld Enviroment\n","class GridWorld(gym.Env):\n","    metadata = {'render.modes': ['console']}\n","\n","    # Actions available\n","    UP = 0\n","    LEFT = 1\n","    DOWN = 2\n","    RIGHT = 3\n","\n","    def __init__(self, width, height, verbose=False):\n","        super(GridWorld, self).__init__()\n","        self.ACTION_NAMES = [\"UP\", \"LEFT\", \"DOWN\", \"RIGHT\"]                         # Mapping of the actions\n","        self.num_actions = 4\n","\n","        self.width = width\n","        self.height = height\n","        self.size = width * height                                                  # Size of the grid world\n","        self.num_states = self.size                                                 # States are computed (width * height)\n","        self.num_obstacles = int((width + height) /2)                               # Number of obstacles present in our Grid\n","        self.end_state = np.array([height - 1, width - 1], dtype=np.uint8)          # Goal state = Bottom right cell\n","\n","        self.action_space = spaces.Discrete(4)                                      # Actions space of agent : up, down, left and right\n","        self.observation_space = spaces.MultiDiscrete([self.height, self.width])    # Observation : Cell indices in the grid\n","\n","        self.obstacles = np.zeros((height, width))                                  # Initialize the obstacles as a Grid with all zeros\n","\n","        self.verbose = verbose                                                      # If we want to print out stuff\n","\n","        \"\"\"\n","        Looping for the number of obstacles, we take a random number between height and width\n","        and until the two numbers are not (0, 0) -> Because it's the goal, we keep on looping.\n","        When we've found two suitable numbers, we put 1 in the Obstacles grid at coords height/width.\n","        \"\"\"\n","        for i in range(self.num_obstacles):\n","            obstacle = random.randrange(height) , random.randrange(width)\n","            while obstacle == (0, 0):\n","                obstacle = random.randrange(height), random.randrange(width)\n","            self.obstacles[obstacle] = 1\n","\n","        self.num_steps = 0\n","        self.max_steps = height * width\n","\n","        self.current_state = np.zeros((2), np.uint8) # Initial state = [0,0]\n","\n","        self.directions = np.array([\n","            [-1, 0], # UP\n","            [0, -1], # LEFT\n","            [1, 0],  # DOWN\n","            [0, 1]   # RIGHT\n","        ])\n","\n","    def transition_function(self, s, a):\n","        s_prime = s + self.directions[a, :]\n","\n","        # Now we check if the agent is going out of the boundaries.\n","        #   - If I take s_prime[0], I'm working on the rows.\n","        #   - If I take s_prime[1], I'm working on the columns.\n","        if(s_prime[0] < 0 or s_prime[0] >= self.height):\n","            if(self.verbose):\n","                print(\"Agent is going outside of the grid. Staying in the same cell\")\n","            return s\n","        if(s_prime[1] < 0 or s_prime[1] >= self.width):\n","            if(self.verbose):\n","                print(\"Agent is going outside of the grid. Staying in the same cell\")\n","            return s\n","        \n","        # Check obstacles for the agent.\n","        # If the agent new coordinates are in the coordinates of an obstacle, exit.\n","        if(self.obstacles[s_prime[0], s_prime[1]] == 1):\n","            print(\"Agent is hitting an obstacle. Staying in the same cell\")\n","            return s            \n","    \n","        return s_prime # We simply return s_prime\n","\n","    def transition_probabilities(self, s, a):\n","        prob_next_state = np.zeros((self.height, self.width))\n","        s_prime = self.transition_function(s, a)\n","\n","        # In a Deterministic environment, the probability of ending up in the next state will always be 1.0\n","        # This doesn't happen in a non-deterministic environment, which we'll cover in the lower part.\n","        prob_next_state[s_prime[0], s_prime[1]] = 1.0\n","\n","        return prob_next_state#.flatten()\n","\n","    def reward_probabilities(self):\n","        rewards = np.zeros((self.num_states)) # Initialize a reward array with shape (1, 15)\n","        i = 0\n","\n","        # For all cells\n","        for r in range(self.height):    # Rows\n","            for c in range(self.width): # Columns\n","                state = np.array([r,c], dtype=np.uint8) # We create an array object in order to pass it to the reward function\n","                rewards[i] = self.reward_function(state) # This reward function will return 1 only in cell (4, 2).\n","                i+=1\n","\n","        return rewards\n","\n","    def reward_function(self,s):\n","        r = 0\n","\n","        # We check if both elements of the arrays s and self.end_state are equal -> (4, 2)\n","        if(s == self.end_state).all():\n","            r = 1\n","\n","        return r\n","\n","    def termination_condition(self, s):\n","\n","        truncated = self.num_steps >= self.max_steps\n","        terminated = False\n","        \n","        if(truncated):\n","            print(\"Maximum steps reached. Exiting.\")\n","            terminated = True\n","        if(s == self.end_state).all():\n","            print(\"Agent is in the goal state. Let's end the loop.\")\n","            terminated = True\n","\n","        return terminated, truncated\n","\n","    def step(self, action):\n","        s_prime = self.transition_function(self.current_state, action)\n","        reward = self.reward_function(s_prime)\n","        terminated, truncated = self.termination_condition(s_prime)\n","\n","        self.current_state = s_prime\n","        self.num_steps += 1\n","\n","        return self.current_state, reward, terminated, truncated, None\n","\n","    def render(self):\n","        '''\n","            Render the state\n","        '''\n","\n","        row = self.current_state[0]\n","        col = self.current_state[1]\n","\n","        for r in range(self.height):\n","            for c in range(self.width):\n","                if r == row and c == col:\n","                    print(\"| A \", end='')\n","                elif r == self.end_state[0] and c == self.end_state[1]:\n","                    print(\"| G \", end='')\n","                else:\n","                    if self.obstacles[r,c] == 1:\n","                        print('|///', end='')\n","                    else:\n","                        print('|___', end='')\n","            print('|')\n","        print('\\n')\n","\n","    def reset(self):\n","        self.current_state = np.zeros((2), np.uint8)\n","        self.num_steps = 0\n","        return self.current_state\n","\n","    def close(self):\n","        pass\n","\n","\n","class NonDeterministicGridWorld(GridWorld):\n","    def __init__(self, width, height, verbose=False, p=0.8):\n","        super(NonDeterministicGridWorld, self).__init__(width, height)\n","        self.probability_right_action = p\n","        self.verbose = verbose\n","\n","    def transition_function(self, s, a):\n","        if(self.verbose):\n","            print(\"Original action to perform:\", self.ACTION_NAMES[a])\n","        s_prime = s + self.directions[a, :]\n","\n","        # With probability 1 - p, we have a diagonal movement\n","        # random.random() returns a number between 0 and 1\n","        if random.random() <= 1 - self.probability_right_action:\n","            if random.random() < 0.5:\n","                if(self.verbose):\n","                    print(F\"Actual action performed: {self.ACTION_NAMES[a]} + {self.ACTION_NAMES[(a+1) % self.num_actions]}\")\n","                s_prime = s_prime + self.directions[(a+1) % self.num_actions, :]\n","            else:\n","                if(self.verbose):\n","                    print(F\"Actual action performed: {self.ACTION_NAMES[a]} + {self.ACTION_NAMES[(a-1) % self.num_actions]}\")\n","                s_prime = s_prime + self.directions[(a-1) % self.num_actions, :]\n","\n","        # Check if the agent goes out of the grid along with obstacles \n","        if s_prime[0] < self.height and s_prime[1] < self.width and (s_prime >= 0).all():\n","            if self.obstacles[s_prime[0], s_prime[1]] == 0 :\n","                return s_prime\n","\n","        return s\n","\n","    def transition_probabilities(self, s, a):\n","        cells = []\n","        probs = []\n","        prob_next_state = np.zeros((self.height, self.width)) # Initialize the probabilities array with all zeros\n","\n","        # We apply our new direction to our current state\n","        # These are our new coordinates in the GridWorld\n","        s_prime_right =  s + self.directions[a, :]\n","\n","        # Usual check if we are hitting the walls or an obstacle\n","        if(self.verbose):\n","            print(F\"Trying to go: {self.ACTION_NAMES[a]}\")\n","        if s_prime_right[0] < self.height and s_prime_right[1] < self.width and (s_prime_right >= 0).all() and self.obstacles[s_prime_right[0], s_prime_right[1]] == 0:\n","                if(self.verbose):          \n","                    print(\"Success!\")\n","                # At this point, we haven't hit a wall or an obstacle, so we say that the probability\n","                # of going to our new coordinates is equal to 0.8\n","                prob_next_state[s_prime_right[0], s_prime_right[1]] = self.probability_right_action\n","                cells.append(s_prime_right)\n","                probs.append(self.probability_right_action)\n","        else:\n","            if(self.verbose):\n","                print(\"Obstacle present.\")\n","\n","        if(self.verbose):\n","            print(\"--------------------\")\n","        # If we have hit an obstacle or a wall using our original action, that's where we try\n","        # the action (direction) that comes just after the one we computed -> a + 1\n","        # One thing to notice is that we SUM the previous action with this new action\n","        #\n","        # ============================================================\n","        # The division by 2 in the line prob_next_state[s_prime[0], s_prime[1]] = (1 - self.probability_right_action) / 2\n","        # is used to evenly distribute the remaining probability among the two diagonal directions when there is a \n","        # non-deterministic action. This is due to the fact that we have two alternative actions (a + 1 / a - 1) \n","        # along with the original action (a)\n","        # ============================================================\n","        s_prime = s_prime_right + self.directions[(a + 1) % self.num_actions, :]\n","\n","        if(self.verbose):\n","            print(F\"Trying to go: {self.ACTION_NAMES[(a + 1) % self.num_actions]}\")\n","        # Usual check if we are hitting the walls or an obstacle\n","        if s_prime[0] < self.height and s_prime[1] < self.width and (s_prime >= 0).all() and self.obstacles[s_prime[0], s_prime[1]] == 0:                \n","                # With our new action (original + new action), we haven't hit anything at all\n","                # so let's say that the probability of going to our new coordinates is equal to:\n","                # (1 - 0.8) / 2 = 0.1\n","                if(self.verbose):\n","                    print(\"Success!\")\n","                prob_next_state[s_prime[0], s_prime[1]] = (1 - self.probability_right_action) / 2\n","                cells.append(s_prime.copy())\n","                probs.append((1 - self.probability_right_action) / 2)\n","        else:\n","            if(self.verbose):\n","                print(\"Obstacle present.\")\n","\n","        if(self.verbose):\n","            print(\"--------------------\")\n","        # If we have hit an obstacle or a wall using our original action, that's where we try\n","        # the action (direction) that comes just before the one we computed -> a - 1\n","        # One thing to notice is that we SUM the previous action with this new action\n","        # In this specific case, though, we are still using our original \"s_prime_right\" action and\n","        # not \"s_prime\". So we are not using the a + 1 action, but the very original first action we computed.\n","        s_prime = s_prime_right + self.directions[(a - 1) % self.num_actions, :]\n","\n","        if(self.verbose):\n","            print(F\"Trying to go: {self.ACTION_NAMES[(a - 1) % self.num_actions]}\")\n","        # Usual check if we are hitting the walls or an obstacle\n","        if s_prime[0] < self.height and s_prime[1] < self.width and (s_prime >= 0).all() and self.obstacles[s_prime[0], s_prime[1]] == 0:\n","                # With our new action (original + new action), we haven't hit anything at all\n","                # so let's say that the probability of going to our new coordinates is equal to:\n","                # (1 - 0.8) / 2 = 0.1\n","                if(self.verbose):\n","                    print(\"Success!\")\n","                prob_next_state[s_prime[0], s_prime[1]] = (1 - self.probability_right_action) / 2\n","                cells.append(s_prime.copy())\n","                probs.append((1 - self.probability_right_action) / 2)\n","        else:\n","            if(self.verbose):\n","                print(\"Obstacle present.\")\n","\n","        if(self.verbose):\n","            print(\"--------------------\")\n","        # Normalization\n","        # We sum up the probabilities and we say that the probability of staying in our original coordinates is:\n","        # (1 - sum of probabilities)\n","        sump = sum(probs) \n","        prob_next_state[s[0], s[1]] = 1 - sump\n","        \n","        return prob_next_state"]},{"cell_type":"markdown","id":"602977ce","metadata":{"id":"602977ce"},"source":["### Applying Value iteration\n","In order to apply Value Iteration, we need the **transition probabilities** and the **reward function**.<br>\n","In this piece of code, we simply print the probability over the next state in a Non-deterministic GridWorld"]},{"cell_type":"code","execution_count":5,"id":"378bbbdd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":235,"status":"ok","timestamp":1694424322193,"user":{"displayName":"Andrea Fanti","userId":"12083130687901228611"},"user_tz":-120},"id":"378bbbdd","outputId":"0de52b71-f7fc-4f36-8a61-4a15e00d63c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["| A |___|___|\n","|///|___|___|\n","|///|___|___|\n","|___|___|___|\n","|///|___| G |\n","\n","\n","Probabilities:\n","[[0.9 0.  0. ]\n"," [0.  0.1 0. ]\n"," [0.  0.  0. ]\n"," [0.  0.  0. ]\n"," [0.  0.  0. ]]\n"]}],"source":["env = NonDeterministicGridWorld(3,5, verbose=False)\n","state = env.reset()\n","env.render()\n","\n","# Now we want to print what are the probabilities of our state if we:\n","# - Start from state [0, 0]\n","# - Perform action \"DOWN\"\n","action = \"DOWN\"\n","next_state_prob = env.transition_probabilities(state, env.ACTION_NAMES.index(action)) # Get index of \"action\" from ACTION_NAMES list\n","\n","print(\"Probabilities:\")\n","print(next_state_prob)"]},{"cell_type":"markdown","id":"fd61c23d","metadata":{"id":"fd61c23d"},"source":["### Reward values"]},{"cell_type":"code","execution_count":6,"id":"d9826985","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":233,"status":"ok","timestamp":1694424325488,"user":{"displayName":"Andrea Fanti","userId":"12083130687901228611"},"user_tz":-120},"id":"d9826985","outputId":"e84d6fc2-6d0a-4b8a-e1bd-15d575f3194f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"]}],"source":["print(env.reward_probabilities())\n"]},{"cell_type":"markdown","id":"c71dea30","metadata":{"id":"c71dea30"},"source":["# Value Iteration algorithm"]},{"cell_type":"markdown","id":"c67f1654","metadata":{},"source":["### Pseudocode"]},{"cell_type":"markdown","id":"ac55c718","metadata":{},"source":["![](imgs/value_iteration.png)"]},{"cell_type":"markdown","id":"1709fd94","metadata":{},"source":["## Implementation"]},{"cell_type":"code","execution_count":7,"id":"408efa64","metadata":{"executionInfo":{"elapsed":226,"status":"ok","timestamp":1694424328154,"user":{"displayName":"Andrea Fanti","userId":"12083130687901228611"},"user_tz":-120},"id":"408efa64"},"outputs":[],"source":["def value_iteration(env, gamma=0.99, iters=100):\n","    \n","    # Initialize values\n","    values = np.zeros((env.num_states))                     # We initialize an array \"values\" with all zeros and shape as num_states (3x5 -> 15)\n","    best_actions = np.zeros((env.num_states), dtype=int)    # Array for storing the best actions\n","    STATES = np.zeros((env.num_states, 2), dtype=np.uint8)  # We redefine the STATES as a collection of rows (15). Each row has 2 values definining that state\n","                                                            # STATES[0] = [0, 0]\n","                                                            # STATES[1] = [0, 1]\n","                                                            # ...\n","                                                            # STATES[14] = [4, 2] -> Goal\n","    REWARDS = env.reward_probabilities()                    # We get a REWARD array with 1 only in the last cell\n","\n","    # Popolate the STATES array\n","    i = 0\n","    for r in range(env.height):\n","        for c in range(env.width):\n","            state = np.array([r, c], dtype=np.uint8)\n","            STATES[i] = state\n","            i += 1\n","\n","    for i in range(iters):\n","        v_old = values.copy() \n","        for s in range(env.num_states): # Looping through all the states i-times.\n","            state = STATES[s] # Take that specific state \"s\"\n","\n","            # If we reach the termination condition, we cannot perform any action\n","            # Termination condition = Either goal or max_steps reached\n","            if (state == env.end_state).all() or i >= env.max_steps:\n","                continue\n","            \n","            max_va = -np.inf\n","            best_a = 0\n","\n","            # For all actions, we see which one is the best in this specific state \"s\".\n","            for a in range(env.num_actions):\n","                next_state_prob = env.transition_probabilities(state, a).flatten() # T(s, a, s')\n","\n","                # Important thing to notice.\n","                # We are working directly on ALL the array, not one by one.\n","                # We could've done it, but it would've been a mess with indices and stuff.\n","                # So we stack everything together and at the end we sum all the values we got\n","                # in this (1, 15) array. The sum will be the formula up to the summation symbol.\n","                va = next_state_prob * (REWARDS + gamma * v_old)\n","                va = va.sum() # We sum the values to get the final exact value, \n","                              # we don't want to return a (1, 15) array.\n","\n","                if va > max_va:\n","                    max_va = va # max over the states (v*_i+1)\n","                    best_a = a  # argmax over the action (pi*_i+1)\n","            values[s] = max_va\n","            best_actions[s] = best_a\n","\n","    return values.reshape((env.height, env.width)), best_actions.reshape((env.height, env.width))"]},{"cell_type":"markdown","id":"33dcbb0d","metadata":{"id":"33dcbb0d"},"source":["### Estimate values"]},{"cell_type":"code","execution_count":8,"id":"1dfa419c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":220,"status":"ok","timestamp":1694424330079,"user":{"displayName":"Andrea Fanti","userId":"12083130687901228611"},"user_tz":-120},"id":"1dfa419c","outputId":"49fc631b-993a-4286-aee4-1ee499832252"},"outputs":[{"name":"stdout","output_type":"stream","text":["Best values: \n"," [[0.94825225 0.95777204 0.96217993]\n"," [0.95819816 0.9678848  0.97361562]\n"," [0.96815382 0.97791984 0.98537072]\n"," [0.97791984 0.98781902 0.99750623]\n"," [0.98537072 0.99750623 0.        ]]\n","Best actions: \n"," [[3 2 2]\n"," [3 2 2]\n"," [2 2 2]\n"," [3 2 2]\n"," [3 3 0]]\n"]}],"source":["values, best_actions = value_iteration(env)\n","\n","print(\"Best values: \\n\", values)\n","print(\"Best actions: \\n\", best_actions)"]},{"cell_type":"markdown","id":"0a68ae40","metadata":{"id":"0a68ae40"},"source":["### Simulate the Optimal policy\n"]},{"cell_type":"code","execution_count":9,"id":"edfe0c92","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":307,"status":"ok","timestamp":1694424358775,"user":{"displayName":"Andrea Fanti","userId":"12083130687901228611"},"user_tz":-120},"id":"edfe0c92","outputId":"c1c3fb82-c4ec-493f-a427-a514dca7a7b7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial state: [0, 0]\n","\n","| A |___|___|\n","|///|___|___|\n","|///|___|___|\n","|___|___|___|\n","|///|___| G |\n","\n","\n","|___| A |___|\n","|///|___|___|\n","|///|___|___|\n","|___|___|___|\n","|///|___| G |\n","\n","\n","|___|___|___|\n","|///| A |___|\n","|///|___|___|\n","|___|___|___|\n","|///|___| G |\n","\n","\n","|___|___|___|\n","|///|___|___|\n","|///| A |___|\n","|___|___|___|\n","|///|___| G |\n","\n","\n","|___|___|___|\n","|///|___|___|\n","|///|___|___|\n","|___| A |___|\n","|///|___| G |\n","\n","\n","|___|___|___|\n","|///|___|___|\n","|///|___|___|\n","|___|___|___|\n","|///| A | G |\n","\n","\n","Agent is in the goal state. Let's end the loop.\n","|___|___|___|\n","|///|___|___|\n","|///|___|___|\n","|___|___|___|\n","|///|___| A |\n","\n","\n"]}],"source":["done = False\n","state = env.reset()\n","print(\"Initial state: [0, 0]\\n\")\n","env.render()\n","while not done:\n","    action = best_actions[state[0],state[1]]\n","\n","    state, reward, terminated, truncated, _ = env.step(action)\n","    done = terminated or truncated\n","    env.render()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":5}
